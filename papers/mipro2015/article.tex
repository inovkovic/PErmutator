\documentclass[11pt,final,conference,a4paper]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[noadjust]{cite}
\usepackage{url}
\usepackage[dvips]{graphicx}

\begin{document}

\IEEEoverridecommandlockouts

\title{Fine grained permutation of text code in PE files}

\author{
	\authorblockN{Bruno Humić, Stjepan Groš}
	\authorblockA{
		Faculty of Electrical and Computing Engineering \\
		University of Zagreb\\
		Unska bb, 10000 Zagreb, Croatia \\
		E-Mail: bruno.humic@gmail.com, stjepan.gros@fer.hr}}

\maketitle

\begin{abstract}
Memory corruption is one of the oldest security vulnerabilities
that is still very common today. It allows attackers to gain full
control of victims computer. Crucial in the step of gaining control
is for attacker to know memory layout of a process. There are a
number of protection mechanisms developed to counter this threat,
one of which are randomization mechanisms that make process image
layout unpredictable for attacker. Randomization based security
mechanisms had a major impact on dealing with memory corruption
vulnerabilities but the threat still remains. In this paper we
describe work started on a new randomization security mechanism
whose goal is to rearrange memory blocks in the code section of a
PE file. In that way, we strive to achieve higher security levels
of a system by making process layout even more unpredictable for
the attackers.
\end{abstract}

\begin{keywords}
PE file format, computer security, operating systems, ASLR, DEP, ASLP, randomization, memory corruption, permutation, control flow graph, Microsoft Windows
\end{keywords}

\section{Introduction}
\label{sec:intro}

Species in the nature use diversity in order to combat different
threats. In other words, if a threat manages to infect one specimen
it want be able to infect many others because all of them are
different. In computer networks, when a threat compromises one
machine, it will probably compromises many others too because they
are all identical, or almost identical.

This is a know premise, and there are attempts to apply principles
from the nature in a computer networks. Yet, we are still far away
from a satisfactory solution.

The paper is structured as follows. First, we review related work
in Section \ref{sec:architecture}. We also list shortcomings of
existing approaches in that section. Then, in Section \ref{sec:design}
we describe the idea behind our solution, and its design and
implementation. In Section \ref{sec:testing} we give results of
different tests we performed on PErmutator. Finally, we conclude
paper in Section \ref{sec:conclusions}.

\section{Related work}
\label{sec:architecture}
Numerous projects and research has been done in the area of software diversity and we will use this section to discuss and describe some of them.

Before going to concrete examples of software diversity, we should note that there are two approaches to the process of software diversification:
\begin{itemize}
\item Pre-distribution approach: The main idea behind this approach is that all diversification action takes place before the program is shipped to end users. Therefore, the majority of diversification in this approach happens in the compiler. Certain advantages of this approach are as follows:
\begin{itemize}
\item Portability: Compilers are tools that are highly sophisticated and costly to produce and maintain. The reason for this is that compilers support multiple instruction sets which allows them to produce end user programs for various platforms. By adding the diversification feature to a compiler, diversified programs can be available on all platforms that are supported by the compiler.
\item Wide transformation range: Since all compilers do sequences of code transformation passes in which instructions are selected and scheduled, it is relatively easy to add randomization transformations to existing passes. Also, the compilers avoid the need for disassembly. This is a significant advantage because the transformation from source code to object code is lossy which makes the perfect recovery of the original program control flow impossible.
\end{itemize}
However, there are some disadvantages when talking about compiler based diversification. If all the diversification happens before the product is shipped, there is an increased cost in the distribution because every client must download a separate and unique product variant. This requires a distribution system which must maintain a large inventory of program variants such that downloads start without delay. Also, the big problem with this approach is patching. Since every client has an unique program variant, the vendor must supply patches that are customized for each individual copy.
\item Post-distribution approach: In this approach all the diversification happens at the clients. The main advantage of this approach is that there is no need to change the current way in which the programs are distributed to clients. Also, when using this approach vendors avoid the cost of diversifying each program copy. However, the client systems must be powerful enough to run the diversification engine.

The disadvantages of this approach are as follows:
\begin{itemize}
\item Client Side attacks: The diversification process on the client can be disabled by malware.
\item Same diversification engine: Since all diversification happens on the client, the client must contain a copy of the engine itself. This allows attackers to analyze the engine and make it possible for attacks.
\end{itemize}
\end{itemize}
In the remaining part of this section we discuss some concrete diversification implementations. Since there are many diversification projects out there, we will be focusing on those that are highly in use today and those which share some common things with the diversification tool we are building.

The most popular randomization security mechanism today is probably ASLR. In fact, it is the only diversified security mechanism that is currently deployed. Before ASLR, the base of code and data segments were always loaded at the same virtual memory addresses. The way ASLR works is that it randomizes the base address of code and data segments. This means that the stack, heap, statically allocated data and the images base address are randomly chosen. By doing this, ASLR significantly aggravates memory corruption, code injection and code reuse attacks. ASLR can be classified as a compiler based or pre-distribution diversity mechanism. The reason for this is because ASLR depends on the compiler which must generate position independent code so that the base address randomization can be carried out.

However, ASLR has certain drawbacks. It can be easily bypassed via information leakage. Also it has very low entropy on 32-bit system which means it can be defeated via brute-force attacks. The reason for low entropy in ASLR is due to the fact that it randomizes only the two high order bytes of the address. Another major disadvantage of ASLR is the fact that it only allows coarse grained permutation. What this means is that ASLR will shift all the addresses by the same amount so that relative distances inside the image remain unchanged. This allows attackers to infer the entire memory layout of the process just by disclosing a single code address.

Another interesting project in software diversity is Address Space Layout Permutation (ASLP). The fine grained permutation tool that we are creating here is very similar to ASLP with the difference that ASLP is focused on the Linux platform. ASLP is mostly a post-distribution diversification mechanism because it's main part is a binary rewriting tool that operates in the user space. Also, ASLP provides both user and kernel level randomization.

The created binary rewriting tool is the foundation for the user level randomization process. This tool comprises of two major phases:
\begin{enumerate}
\item Coarse grained permutation: The goal of this phase is to shift the code and data segments according to the offset values given by the user.
\item Fine grained permutation: The goal of fine grained permutation in the ASLP project is to randomly change the order of functions and variables inside the code and data segments.
\end{enumerate}
In the kernel level permutation part, ASLP is very similar to ASLR. Just like ASLR, ASLP randomizes the location of stack, heap and shared libraries. In order to do so, the Linux kernel had to be modified. This is also the part where our fine grained permutation tool differs from ASLP because we aren't providing diversification on the kernel level. This is due to the fact that we are creating a tool for Microsoft Windows operating system whose kernel is closed to outside modifications.
The last thing to note about ASLP is that it has very low performance overhead (less than 1\%) and it allows randomization of 20 bits in the addresses of code, data and shared library segments.

In order for this section to be complete, we would like to draw attention to one more pre-distribution based diversification mechanism. 

It is well known that most compilers can execute a program to discover "hot" and "cold" code paths. Also, it is well known that diversification tends to make programs run slower. This brings up the idea that compilers can be used in the diversification process in a way to reduce the amount of diversification for "hot" code parts and direct the diversification towards "cold" code parts. By doing so, the performance overhead is significantly lowered.

Homescu et al. used this fact to create a profile guided diversity engine based on NOP insertion. The main idea behind the project is to insert most NOP instruction inside the "cold" code area. In that way the target binary gets randomized but the performance overhead is kept to a minimum. 


\section{PErmutator design and implementation}
\label{sec:design}

At this moment, our fine grained permutation engine is following a post-distribution diversification approach. The main part of the project is a binary rewriting tool which performs basic block reordering. This binary rewriting tool can be divided into several key parts:
\begin{enumerate}
\item Disassembler
\item PE format analysis functions
\item Control flow graph creation
\item Permutation engine
\end{enumerate}
In the remaining part of this section we describe each of the key parts mentioned above as well as the implementation problems we are currently facing.

\subsection{Disassembler}
\label{sec:disassembler}
When creating a post-distribution diversification engine it is necessary to include a disassembly engine in the project. In order to obtain basic blocks from the PE file text section, we have to use a disassembler because it allows us to get human readable representation of raw bytes from the text section. This is important because doing so we can easily identify free branches such as return, call and jump instructions. Once those instructions are discovered, the process of control flow graph construction can begin \ref{sec:cfg}.

Creating a disassembler from scratch is very challenging. This is due to the fact that there are hundreds of possible processor instructions for the x86 platform and it would take a considerable amount of time to build a fast and reliable disassembler. Luckily, there are some very high quality disassembler libraries that can be used. Since we decided to build our fine grained permutation engine in C++, we determined that the \emph{distorm} disassembler library would be a perfect fit. \emph{Distorm} is a fast and powerful disassembler library that offers disassembler functionality for a wide range of programming languages. Also, every disassembled instruction is stored in a structure which makes it easy to access certain parts of each instruction.

\subsection{PE format analysis functions}
\label{sec:pe_functions}
Our fine grained permutation engine operates on the content of the text section in a PE file. Before we can give the data from the text section to a disassembler, we firstly need to extract that data from the rest of the PE file. Also, when doing a binary rewriting process on a PE file, certain values in the file headers need to be modified. For example, the entry point of the PE file needs to be changed, the size of image needs to be updated because new instructions will be added to the text section. By adding new instructions, the text section size gets changed so we need to update the section header accordingly.

To make all those operations as simple as possible, we developed a set of functions which allows us to easily access and modify all the parts from the PE file. All those functions can be used for other projects related to PE file format and are available at the projects Github page.

\subsection{Control flow graph}
\label{sec:cfg}
When developing a fine grained permutation engine that is based on basic block reordering, it is necessary to create a control flow graph. Having an in memory representation of a control flow graph makes the process of basic block reordering significantly easier because all the basic blocks can be easily reached and moved to an arbitrary position in the text section of a PE file. In the remaining part of this section, there is a detailed description of our recursive algorithm that we used for creating the control flow graph.

In order to successfully create a control flow graph, we first need to extract the text section from the PE file. This can be easily done by using specific PE functions we talked about in section \ref{sec:pe_functions}. Once the text section is extracted, we position at the programs entry point which can be read from the header of a PE file. At this point, the disassembly process begins which allows us to easily identify the free branches such as return and jump instructions. 

Once we obtain the disassembly of a text section, we need to look for the first occurrence of the following instructions: RET, RETN, DB or a jump instruction. At this point there are two possible steps the program takes, depending on the free branch instruction we encountered:
\begin{enumerate}
\item If a read instruction is RET, RETN or DB, we mark its position relative to the beginning of the text section. At this point a new node is constructed and added to the graph. That node represents a basic block. Also, the node contains all of the instructions from the beginning of the current block to the position of the RET, RETN or DB instruction. Since this block ends with an instruction such as RET, RETN or DB we mark is an end node in a control flow graph. This means that this node has no child nodes and the recursion level decreases.
\item If a jump instruction is read, the process is similar to the above step. A new node is constructed and added to the graph, but keeping in mind that the jump instruction is the last instruction in the node. Two things can happen at this point:
\begin{enumerate}
\item Direct jump: If the last instruction is a direct jump, the recursion will increase for one level because that node will have one child node which is the destination of the direct jump.
\item Indirect jump: If the last instruction is an indirect jump, there will be two more recursive calls. One call will be in the case the jump takes place, and the other in case the jump doesn't get executed. This way, the node that ends in an indirect jump instruction will have two child nodes.
\end{enumerate}
\end{enumerate}
In order to check if the algorithm works correctly, we implemented a graph description generator for the \emph{graphviz} program. By doing so, we were able to visualize the graph and make sure that it is correct.

\subsection{Permutation engine}
\label{sec:permutator}
Permutation engine is the most important part of the project. Its task is to use the basic blocks obtained in section \ref{sec:cfg} and reorder them in a way that a new memory layout for the PE file is created but that the functionality of the PE file stays the same. 

This is the most challenging part of the entire projects and it still has certain implementation problems. The biggest problem we encounter in this part is the task of updating all the references between the basic blocks. For example, imagine that a certain block \emph{block\_1} contains the following instruction \emph{PUSH 0x0040102B}. The operand of the \emph{PUSH} instruction can be interpreted in two ways: either it is a constant value, or it is an address. If the operand is a constant value, there is no need to modify it. However, lets say that the operand is an address of a callback function which resides in block \emph{block\_2}. If \emph{block\_2} gets moved to some other place, the given address \emph{0x0040102B} won't be correct anymore. In order for the new PE file to work, the operand in the block \emph{block\_1} needs to be updated accordingly. The problem that arises here lays in the fact that when doing static file analysis on a PE file we can not know for sure whether an operand is a constant or an address. There are a couple of solutions for this problem which we discuss in the remainder of this section.

One way to solve this problem is by using the information in the relocation table of a PE file. The relocation table contains information about all the places in a PE file which need to be modified if the file isn't loaded at the preferred address. The problem with this approach is that not all PE files contain the relocation table. If those PE files get loaded at some other address, rather than the preferred one, the program won't work.

However, we started working on a heuristic solution for the given problem. The idea is to mark every operand that contains data which can be part of the text section as an address. We believe that by doing so, the problem would be solved. This solution is still in implementation phase.

\section{Testing and validation}
\label{sec:testing}

\subsection{Validation}

Running vulnerable application to test if protection is better than ASLR.

\subsection{Performance}

Loading and executing application

Running application performance penalties

\section{Conclusions and Future Work}
\label{sec:conclusions}

\bibliographystyle{IEEEtran}
\bibliography{bibliography}

% Temporary fix until there are some references in the text
\nocite{arscryptolocker}

\end{document}
